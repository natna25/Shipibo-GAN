{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCGAN 128.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"QcydLUwhzxsM","colab_type":"code","colab":{}},"source":["try:\n","    from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU\n","    from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, UpSampling2D, Conv2D,Conv2DTranspose\n","    from tensorflow.keras.models import Sequential, Model\n","    from tensorflow.keras.optimizers import Adam\n","    from tensorflow.keras import backend as K\n","except: \n","    from keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU\n","    from keras.layers import BatchNormalization, Activation, ZeroPadding2D, UpSampling2D, Conv2D,Conv2DTranspose\n","    from keras.models import Sequential, Model\n","    from keras.optimizers import Adam\n","    from keras import backend as K\n","import tensorflow as tf\n","\n","import os\n","import argparse\n","import glob \n","\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","import sys\n","\n","import numpy as np\n","\n","class DCGAN():\n","    def __init__(self, img_rows=128, img_cols=128, channels=3, latent_dim=100, loss='binary_crossentropy', name='shipibo_128'):\n","        self.name = name\n","\n","        # Input shape\n","        self.img_rows = img_rows\n","        self.img_cols = img_cols\n","        self.channels = channels\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","        self.latent_dim = latent_dim\n","        self.loss = loss\n","\n","        self.optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # Build the GAN\n","        self.build_combined()\n","        \n","    def build_combined(self):\n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.latent_dim,))\n","        img = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated images as input and determines validity\n","        valid = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, valid)\n","        self.combined.compile(loss=self.loss, optimizer=self.optimizer)    \n","    \n","    def load_weights(self, generator_file=None, discriminator_file=None):\n","\n","        if generator_file:\n","            generator = self.build_generator()\n","            generator.load_weights(generator_file)\n","            self.generator = generator\n","    \n","        if discriminator_file:\n","            #discriminator = self.build_discriminator()\n","            self.discriminator.load_weights(discriminator_file)\n","            #self.discriminator = discriminator\n","\n","        if generator_file or discriminator_file: \n","            self.build_combined() \n","\n","        print(\"loaded model weights\")\n","\n","    def build_generator(self):\n","\n","        model = Sequential()\n","        #model.add(Dense(128, activation=\"relu\", input_dim=self.latent_dim, name=\"generator_input\") )\n","        #model.add(Dropout(0.1))\n","        \n","        #4,4\n","        model.add(Dense(1024 * 4 * 4, activation=\"relu\", input_dim=self.latent_dim, name=\"generator_input\") )\n","        model.add(Dropout(0.35))\n","        model.add(Reshape((4, 4, 1024)))\n","        #model.add(UpSampling2D())\n","\n","        #8,8\n","        model.add(Conv2DTranspose(512, kernel_size=4,strides=(2,2), padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.30))\n","        \n","\n","        16,16\n","        model.add(Conv2DTranspose(256, kernel_size=4,strides=(2,2), padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        \n","\n","        #32,32\n","        model.add(Conv2DTranspose(128, kernel_size=4,strides=(2,2), padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","\n","        #64,64\n","        model.add(Conv2DTranspose(64, kernel_size=4,strides=(2,2), padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.20))\n","        \n","        #128,128\n","        model.add(Conv2DTranspose(self.channels, kernel_size=4,strides=(2,2), padding=\"same\", activation=\"tanh\", name=\"generator_output\"))\n","\n","        model.summary()\n","\n","        noise = Input(shape=(self.latent_dim,))\n","        img = model(noise)\n","\n","        return Model(noise, img, name=\"generator\")\n","\n","    def build_discriminator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Conv2D(64, kernel_size=4, strides=2, input_shape=self.img_shape, padding=\"same\"))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.35))\n","\n","        model.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n","        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.35))\n","\n","        model.add(Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.35))\n","\n","        model.add(Conv2D(512, kernel_size=4, strides=2, padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.35))\n","        \n","        \n","        model.add(Conv2D(1024, kernel_size=5, strides=1, padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.30))\n","        \n","        \n","        model.add(Flatten())\n","\n","        #model.add(Dense(32, activation='relu'))\n","        model.add(Dense(1, activation='sigmoid'))\n","\n","        model.summary()\n","\n","        \n","        img = Input(shape=self.img_shape)\n","        validity = model(img)\n","\n","        discrim = Model(img, validity)\n","\n","        \n","\n","        discrim.compile(loss='binary_crossentropy',\n","            optimizer=self.optimizer,\n","            metrics=['accuracy'])\n","\n","        return discrim\n","\n","    \n","    def normalize_images(self,im):\n","        return ((im + 1)/2.0)\n","\n","\n","    def train(self, X_train, epochs, batch_size=128, save_interval=100,batch_sample=5):\n","\n","        # Adversarial ground truths\n","\n","\n","        #fixed noise points for using train images\n","\n","        fixed_noise = np.random.normal(0, 1, ( 9 , self.latent_dim)) # generate 9 points\n","        train_imgs = []\n","\n","        for epoch in range(epochs):\n","            \n","            #noisy labels for improved training\n","            #check if this has an effect on disc accuracy?\n","            valid = np.ones((batch_size, 1)) - np.random.uniform(low=0.0, high=0.1, size=(batch_size, 1))\n","            fake = np.zeros((batch_size, 1)) + np.random.uniform(low=0.0, high=0.1, size=(batch_size, 1))\n","            \n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            # Select a random half of images\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs = X_train[idx]\n","\n","            # Sample noise and generate a batch of new images\n","            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","            gen_imgs = self.generator.predict(noise)\n","\n","            # Train the discriminator (real classified as ones and generated as zeros)\n","            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","\n","            # Train the generator (wants discriminator to mistake images as real)\n","            g_loss = self.combined.train_on_batch(noise, valid)\n","\n","            # Plot the progress\n","            if epoch % 10 == 0:\n","                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","\n","            if epoch % batch_sample == 0:\n","                #work on training animation\n","                epoch_imgs = self.generator.predict(fixed_noise)\n","                train_imgs.append(self.normalize_images(epoch_imgs)) \n","\n","            # If at save interval => save generated image samples\n","            if epoch % save_interval == 0:\n","                self.save_imgs( \"images/{}_{:05d}.png\".format(self.name,epoch) )\n","                # self.combined.save_weights(\"combined_weights ({}).h5\".format(self.name)) # https://github.com/keras-team/keras/issues/10949\n","                self.generator.save(\"generator ({}).h5\".format(self.name))\n","                self.discriminator.save(\"discriminator ({}).h5\".format(self.name))\n","\n","\n","        return train_imgs\n","\n","    def save_models(self):\n","        self.generator.save(\"generator ({}).h5\".format(self.name))\n","        self.discriminator.save(\"discriminator ({}).h5\".format(self.name))\n","\n","    def save_imgs(self, name=''):\n","        r, c = 4, 4\n","        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n","\n","        # replace the first two latent variables with known values\n","        #for i in range(r):\n","        #    for j in range(c):\n","        #        noise[4*i+j][0] = i/(r-1)-0.5\n","        #        noise[4*i+j][1] = j/(c-1)-0.5\n","\n","        gen_imgs = self.generator.predict(noise)\n","        gen_imgs = self.normalize_images(gen_imgs)\n","\n","        fig, axs = plt.subplots(r, c, figsize=(6.72,6.72))\n","        plt.subplots_adjust(left=0.05,bottom=0.05,right=0.95,top=0.95, wspace=0.2, hspace=0.2)\n","\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt])\n","                axs[i,j].axis('off')\n","                cnt += 1\n","\n","        if name:\n","            fig.savefig(name, facecolor='white' )\n","        else: \n","            fig.savefig('{}.png'.format(self.name), facecolor='black' )\n","\n","        plt.close()\n","    \n","\n","def export_model(saver, model, model_name, input_node_names, output_node_name):\n","    from tensorflow.python.tools import freeze_graph\n","    from tensorflow.python.tools import optimize_for_inference_lib\n","    \n","    if not os.path.exists('out'):\n","        os.mkdir('out')\n","\n","    tf.train.write_graph(K.get_session().graph_def, 'out', model_name + '_graph.pbtxt')\n","\n","    saver.save(K.get_session(), 'out/' + model_name + '.chkp')\n","\n","    freeze_graph.freeze_graph('out/' + model_name + '_graph.pbtxt', None, False,\n","                              'out/' + model_name + '.chkp', output_node_name,\n","                              \"save/restore_all\", \"save/Const:0\",\n","                              'out/frozen_' + model_name + '.bytes', True, \"\")\n","\n","    input_graph_def = tf.GraphDef()\n","    with tf.gfile.Open('out/frozen_' + model_name + '.bytes', \"rb\") as f:\n","        input_graph_def.ParseFromString(f.read())\n","\n","    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n","            input_graph_def, input_node_names, [output_node_name],\n","            tf.float32.as_datatype_enum)\n","\n","    with tf.gfile.FastGFile('out/opt_' + model_name + '.bytes', \"wb\") as f:\n","        f.write(output_graph_def.SerializeToString())\n","\n","    print(\"graph saved!\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"niTJ4heF8sKv","colab_type":"code","outputId":"b9d7e9b7-c471-4648-8809-ebf7dc053c96","executionInfo":{"status":"ok","timestamp":1568228289512,"user_tz":-120,"elapsed":27987,"user":{"displayName":"Antoine T","photoUrl":"","userId":"04821810718843294128"}},"colab":{"base_uri":"https://localhost:8080/","height":730}},"source":["#load dataset\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","\n","os.chdir(\"/content/drive/My Drive/GAN/Shipibo GAN/\")\n","!ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"," animation\n"," anim_grid_2.mp4\n"," anim_grid_3.mp4\n"," anim_grid_dcgan_128.mp4\n"," anim_grid_dcgan_64_2.mp4\n"," anim_grid_dcgan_64.mp4\n"," anim_grid.mp4\n","'Attempts and results.gdoc'\n","'Copy of copy of DCGAN artificial art from github 64.ipynb'\n","'copy of DCGAN artificial art from github.ipynb'\n","'DCGAN 128.ipynb'\n","'discriminator (shipibo_128).h5'\n","'discriminator (shipibo_64_black).h5'\n","'discriminator (shipibo_64).h5'\n","'discriminator (shipibo_64_white).h5'\n","'discriminator (shipibo).h5'\n","'generator (shipibo_128).h5'\n","'generator (shipibo_64_black).h5'\n","'generator (shipibo_64).h5'\n","'generator (shipibo_64_white).h5'\n","'generator (shipibo).h5'\n"," imgs_32\n"," imgs_64\n"," preproc\n","'Shipibo GAN 32 V2.ipynb'\n","'Shipibo GAN 64 V2.ipynb'\n"," test_gif_128.mp4\n"," test_gif_2.mp4\n"," test_gif_64.mp4\n"," test_gif_black_64.mp4\n"," test_gif_white_64.mp4\n"," train_grid.mp4\n"," V1\n","'W DCGAN 64.ipynb'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"--tMDwmzwvtk","colab_type":"text"},"source":["https://github.com/googlecolab/colabtools/issues/287#issuecomment-517446876\n","\n","measures when vm wont synch with drive\n","\n"]},{"cell_type":"code","metadata":{"id":"E9pOdnSLwZP7","colab_type":"code","colab":{}},"source":["#drive.flush_and_unmount()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2GotuNDo-Hyj","colab_type":"code","colab":{}},"source":["\n","!mkdir imgs_128\n","!unzip -q preproc/128/0_compressed_128.zip -d imgs_128/\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BaU1AGJRBkkJ","colab_type":"code","colab":{}},"source":["!mkdir images"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mbp2X380-Oq4","colab_type":"code","outputId":"b32a8aa1-c638-4425-ca32-95e9c5fb53bc","executionInfo":{"status":"ok","timestamp":1568228500539,"user_tz":-120,"elapsed":238987,"user":{"displayName":"Antoine T","photoUrl":"","userId":"04821810718843294128"}},"colab":{"base_uri":"https://localhost:8080/","height":586}},"source":["from keras.preprocessing.image import load_img,img_to_array\n","from PIL import Image\n","\n","def load_images(path,size=128,final_size=0):\n","    \"\"\" loads images into a numpy array and returns them\n","    \"\"\"\n","    img_paths = [path + f  for f in os.listdir(path)]\n","\n","    print(len(img_paths))\n","\n","    if final_size== 0:\n","        final_size = len(img_paths)\n","\n","    dataset_shape = (final_size ,size,size,3)\n","\n","    data = np.ndarray(shape=dataset_shape)\n","\n","    for i,f in enumerate(img_paths):\n","        with Image.open(f) as im:\n","            \n","            im = np.array(im)\n","            \n","            if i % 500 == 0:\n","                print(\"loaded \", i,\"images\" )\n","\n","            data[i] = im.astype(int)\n","\n","            if (i+1) % final_size == 0:\n","\n","                return data\n","            \n","    print(data.shape)\n","\n","    return data\n","\n","data = load_images(\"imgs_128/\",final_size=0)\n","\n","#normalize data to tanh dim\n","data  = (data - 127.5) / 127.5"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["14700\n","loaded  0 images\n","loaded  500 images\n","loaded  1000 images\n","loaded  1500 images\n","loaded  2000 images\n","loaded  2500 images\n","loaded  3000 images\n","loaded  3500 images\n","loaded  4000 images\n","loaded  4500 images\n","loaded  5000 images\n","loaded  5500 images\n","loaded  6000 images\n","loaded  6500 images\n","loaded  7000 images\n","loaded  7500 images\n","loaded  8000 images\n","loaded  8500 images\n","loaded  9000 images\n","loaded  9500 images\n","loaded  10000 images\n","loaded  10500 images\n","loaded  11000 images\n","loaded  11500 images\n","loaded  12000 images\n","loaded  12500 images\n","loaded  13000 images\n","loaded  13500 images\n","loaded  14000 images\n","loaded  14500 images\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tmzDmYTHrXcK","colab_type":"code","colab":{}},"source":["from matplotlib import animation\n","\n","def animate_single_frame(train_imgs,index=0):\n","    \n","\n","    ims = []\n","    fig = plt.figure()\n","    ax = fig.add_axes([0,0,1,1], frameon=False, aspect=1)\n","    ax.set_xticks([]); ax.set_yticks([])\n","\n","    for train_ims in train_imgs:\n","\n","        \n","\n","        im_1 = plt.imshow(train_ims[0,:,:],vmin=0, vmax=1, animated=True) #add first image for test\n","        \n","        ims.append([im_1])\n","        #plt.pause(0.1) \n","\n","    mp4_writer =  animation.writers['ffmpeg']\n","    writer = mp4_writer(fps=24, metadata=dict(artist='Me'), bitrate=1800)\n","\n","\n","    anim = animation.ArtistAnimation(fig,ims)\n","    anim.save(\"test_gif_128.mp4\", writer= writer)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uBZcqCZeCjOU","colab_type":"code","outputId":"cb03b5f3-ccd5-4f1c-893b-3b577dc0a983","executionInfo":{"status":"ok","timestamp":1568228515125,"user_tz":-120,"elapsed":253559,"user":{"displayName":"Antoine T","photoUrl":"","userId":"04821810718843294128"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["preload_model = True\n","\n","dcgan =  DCGAN(latent_dim=128) #create class\n","\n","if preload_model:\n","    dcgan.load_weights(generator_file=\"generator (shipibo_128).h5\",discriminator_file=\"discriminator (shipibo_128).h5\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 64, 64, 64)        3136      \n","_________________________________________________________________\n","leaky_re_lu (LeakyReLU)      (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 32, 32, 128)       131200    \n","_________________________________________________________________\n","zero_padding2d (ZeroPadding2 (None, 33, 33, 128)       0         \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 33, 33, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 33, 33, 128)       0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 33, 33, 128)       0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 17, 17, 256)       524544    \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 17, 17, 256)       1024      \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 17, 17, 256)       0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 17, 17, 256)       0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 9, 9, 512)         2097664   \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 9, 9, 512)         2048      \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 9, 9, 512)         0         \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 9, 9, 512)         0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 9, 9, 1024)        13108224  \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 9, 9, 1024)        4096      \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 9, 9, 1024)        0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 9, 9, 1024)        0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 82944)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 1)                 82945     \n","=================================================================\n","Total params: 15,955,393\n","Trainable params: 15,951,553\n","Non-trainable params: 3,840\n","_________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","generator_input (Dense)      (None, 16384)             2113536   \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 16384)             0         \n","_________________________________________________________________\n","reshape (Reshape)            (None, 4, 4, 1024)        0         \n","_________________________________________________________________\n","conv2d_transpose (Conv2DTran (None, 8, 8, 512)         8389120   \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 8, 8, 512)         2048      \n","_________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)    (None, 8, 8, 512)         0         \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 8, 8, 512)         0         \n","_________________________________________________________________\n","conv2d_transpose_1 (Conv2DTr (None, 16, 16, 256)       2097408   \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 16, 16, 256)       1024      \n","_________________________________________________________________\n","leaky_re_lu_6 (LeakyReLU)    (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","conv2d_transpose_2 (Conv2DTr (None, 32, 32, 128)       524416    \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 32, 32, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","dropout_8 (Dropout)          (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","conv2d_transpose_3 (Conv2DTr (None, 64, 64, 64)        131136    \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 64, 64, 64)        256       \n","_________________________________________________________________\n","leaky_re_lu_8 (LeakyReLU)    (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","dropout_9 (Dropout)          (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","generator_output (Conv2DTran (None, 128, 128, 3)       3075      \n","=================================================================\n","Total params: 13,262,531\n","Trainable params: 13,260,611\n","Non-trainable params: 1,920\n","_________________________________________________________________\n","Model: \"sequential_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","generator_input (Dense)      (None, 16384)             2113536   \n","_________________________________________________________________\n","dropout_10 (Dropout)         (None, 16384)             0         \n","_________________________________________________________________\n","reshape_1 (Reshape)          (None, 4, 4, 1024)        0         \n","_________________________________________________________________\n","conv2d_transpose_4 (Conv2DTr (None, 8, 8, 512)         8389120   \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 8, 8, 512)         2048      \n","_________________________________________________________________\n","leaky_re_lu_9 (LeakyReLU)    (None, 8, 8, 512)         0         \n","_________________________________________________________________\n","dropout_11 (Dropout)         (None, 8, 8, 512)         0         \n","_________________________________________________________________\n","conv2d_transpose_5 (Conv2DTr (None, 16, 16, 256)       2097408   \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 16, 16, 256)       1024      \n","_________________________________________________________________\n","leaky_re_lu_10 (LeakyReLU)   (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","dropout_12 (Dropout)         (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","conv2d_transpose_6 (Conv2DTr (None, 32, 32, 128)       524416    \n","_________________________________________________________________\n","batch_normalization_10 (Batc (None, 32, 32, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_11 (LeakyReLU)   (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","dropout_13 (Dropout)         (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","conv2d_transpose_7 (Conv2DTr (None, 64, 64, 64)        131136    \n","_________________________________________________________________\n","batch_normalization_11 (Batc (None, 64, 64, 64)        256       \n","_________________________________________________________________\n","leaky_re_lu_12 (LeakyReLU)   (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","dropout_14 (Dropout)         (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","generator_output (Conv2DTran (None, 128, 128, 3)       3075      \n","=================================================================\n","Total params: 13,262,531\n","Trainable params: 13,260,611\n","Non-trainable params: 1,920\n","_________________________________________________________________\n","loaded model weights\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kg_unsMP-pRm","colab_type":"code","outputId":"3d8140fa-b567-4519-808c-c627a2162dea","executionInfo":{"status":"error","timestamp":1567807070193,"user_tz":-120,"elapsed":121700,"user":{"displayName":"Antoine T","photoUrl":"","userId":"04821810718843294128"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","\n","train_imgs = dcgan.train(data,epochs=20000,batch_size=64,batch_sample=3)\n","\n","animate_single_frame(train_imgs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0 [D loss: 0.193817, acc.: 0.00%] [G loss: 3.644160]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ohKbuY3eBjx0","colab_type":"code","colab":{}},"source":["\n","noise = np.random.normal(0, 1, ( 9 , dcgan.latent_dim)) # generate 9 points\n","imgs = dcgan.normalize_images(dcgan.generator.predict(noise))\n","\n","for i in range(len(noise)):\n","\n","    plt.subplot(3,3, i+1)\n","    plt.imshow(imgs[i])\n","\n","plt.show\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSGJssMslKSl","colab_type":"code","colab":{}},"source":["\n","\n","# starting point for every image\n","seed_start = np.random.normal(0, 1, (9, dcgan.latent_dim))\n","\n","# these parameters will change every time step\n","latentSpeed = np.random.normal(3, 1, (9, dcgan.latent_dim))\n","vary = np.copy(seed_start)\n","\n","# video settings\n","time = 0\n","fps = 30\n","maxTime = 20 # seconds\n","frameCount = 0\n","\n","anim_imgs = []\n","\n","while (time <= maxTime):\n","\n","    # for each image\n","    for i in range(len(seed_start)): \n","        \n","        # change the latent variables\n","        for j in range(dcgan.latent_dim):\n","            vary[i][j] = seed_start[i][j] + np.sin( 2*np.pi*(time/maxTime) * latentSpeed[i][j] ) \n","\n","    gen_imgs = dcgan.generator.predict(vary)\n","\n","    anim_imgs.append(dcgan.normalize_images(gen_imgs))\n","    \n","\n","    frameCount += 1\n","    time += .70/fps"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"peuHxidJlmKJ","colab_type":"code","colab":{}},"source":["from matplotlib import animation\n","\n","fig,axs = plt.subplots(3,3)\n","\n","print(axs.shape)\n","\n","for i in range(axs.shape[0]):\n","    for j in range(axs.shape[1]):\n","        \n","        axs[i,j].set_xticks([])\n","        axs[i,j].set_yticks([])\n","\n","images = []\n","\n","#iterate ove images\n","for imgs in anim_imgs:\n","    ims = [] \n","    for i,img in enumerate(imgs):\n","        \n","        line = i%3\n","        col = int(i/3)\n","        ims.append(axs[line,col].imshow(img[:,:],vmin=0, vmax=1, animated=True))\n","    \n","    images.append(ims)\n","\n","mp4_writer =  animation.writers['ffmpeg']\n","writer = mp4_writer(fps=24, metadata=dict(artist='Me'), bitrate=1800)\n","\n","anim = animation.ArtistAnimation(fig,images)\n","anim.save(\"anim_grid_dcgan_128.mp4\", writer= writer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"daSw0k1Vx1BO","colab_type":"code","outputId":"3132f897-c5c1-49d4-acea-6ebb3b3846a0","executionInfo":{"status":"ok","timestamp":1568235128973,"user_tz":-120,"elapsed":32693,"user":{"displayName":"Antoine T","photoUrl":"","userId":"04821810718843294128"}},"colab":{"base_uri":"https://localhost:8080/","height":618,"output_embedded_package_id":"1xkBWRDhNVaO20rEAoT_aqbvYUrJIlV-B"}},"source":["from IPython.display import HTML\n","from matplotlib import animation\n","\n","ims = []\n","fig = plt.figure() \n","ax = fig.add_axes([0,0,1,1], frameon=False, aspect=1)\n","ax.set_xticks([]); ax.set_yticks([])\n","\n","for train_ims in anim_imgs:\n","\n","    \n","\n","    im_1 = plt.imshow(train_ims[1,:,:],vmin=0, vmax=1, animated=True) #add first image for test\n","    \n","    ims.append([im_1])\n","    #plt.pause(0.1) \n","\n","#mp4_writer =  animation.writers['ffmpeg']\n","#writer = mp4_writer(fps=24, metadata=dict(artist='Me'), bitrate=1800)\n","\n","fps=30\n","\n","anim = animation.ArtistAnimation(fig,ims,interval=(1/fps)*1000)\n","HTML(anim.to_html5_video())"],"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"R4KPSnP_Rjxv","colab_type":"code","colab":{}},"source":["from matplotlib import animation\n","from IPython.display import HTML\n","fig,axs = plt.subplots(3,3)\n","\n","print(axs.shape)\n","\n","for i in range(axs.shape[0]):\n","    for j in range(axs.shape[1]):\n","        \n","        axs[i,j].set_xticks([])\n","        axs[i,j].set_yticks([])\n","\n","images = []\n","\n","#iterate ove images\n","for imgs in anim_imgs:\n","    ims = [] \n","    for i,img in enumerate(imgs):\n","        \n","        line = i%3\n","        col = int(i/3)\n","        ims.append(axs[line,col].imshow(img[:,:],vmin=0, vmax=1, animated=True))\n","    \n","    images.append(ims)\n","\n","\n","\n","fps=30\n","\n","anim = animation.ArtistAnimation(fig,images,interval=(1/fps)*1000)\n","HTML(anim.to_html5_video())"],"execution_count":0,"outputs":[]}]}