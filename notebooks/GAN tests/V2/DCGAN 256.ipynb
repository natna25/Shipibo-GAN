{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCGAN 256.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"QcydLUwhzxsM","colab_type":"code","colab":{}},"source":["try:\n","    from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU\n","    from tensorflow.keras.layers import BatchNormalization, Activation, ZeroPadding2D, UpSampling2D, Conv2D,Conv2DTranspose\n","    from tensorflow.keras.models import Sequential, Model\n","    from tensorflow.keras.optimizers import Adam\n","    from tensorflow.keras import backend as K\n","except: \n","    from keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU\n","    from keras.layers import BatchNormalization, Activation, ZeroPadding2D, UpSampling2D, Conv2D,Conv2DTranspose\n","    from keras.models import Sequential, Model\n","    from keras.optimizers import Adam\n","    from keras import backend as K\n","import tensorflow as tf\n","\n","import os\n","import argparse\n","import glob \n","\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","\n","import sys\n","\n","import numpy as np\n","\n","class DCGAN():\n","    def __init__(self, img_rows=256, img_cols=256, channels=3, latent_dim=100, loss='binary_crossentropy', name='shipibo_256'):\n","        self.name = name\n","\n","        # Input shape\n","        self.img_rows = img_rows\n","        self.img_cols = img_cols\n","        self.channels = channels\n","        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n","        self.latent_dim = latent_dim\n","        self.loss = loss\n","\n","        self.optimizer = Adam(0.0002, 0.5)\n","\n","        # Build and compile the discriminator\n","        self.discriminator = self.build_discriminator()\n","\n","        # Build the generator\n","        self.generator = self.build_generator()\n","\n","        # Build the GAN\n","        self.build_combined()\n","        \n","    def build_combined(self):\n","        # The generator takes noise as input and generates imgs\n","        z = Input(shape=(self.latent_dim,))\n","        img = self.generator(z)\n","\n","        # For the combined model we will only train the generator\n","        self.discriminator.trainable = False\n","\n","        # The discriminator takes generated images as input and determines validity\n","        valid = self.discriminator(img)\n","\n","        # The combined model  (stacked generator and discriminator)\n","        # Trains the generator to fool the discriminator\n","        self.combined = Model(z, valid)\n","        self.combined.compile(loss=self.loss, optimizer=self.optimizer)    \n","    \n","    def load_weights(self, generator_file=None, discriminator_file=None):\n","\n","        if generator_file:\n","            generator = self.build_generator()\n","            generator.load_weights(generator_file)\n","            self.generator = generator\n","    \n","        if discriminator_file:\n","            #discriminator = self.build_discriminator()\n","            self.discriminator.load_weights(discriminator_file)\n","            #self.discriminator = discriminator\n","\n","        if generator_file or discriminator_file: \n","            self.build_combined() \n","\n","        print(\"loaded model weights\")\n","\n","    def build_generator(self):\n","\n","        model = Sequential()\n","        #model.add(Dense(128, activation=\"relu\", input_dim=self.latent_dim, name=\"generator_input\") )\n","        #model.add(Dropout(0.1))\n","        \n","        #4,4\n","        model.add(Dense(1024 * 4 * 4, activation=\"relu\", input_dim=self.latent_dim, name=\"generator_input\") )\n","        model.add(Dropout(0.35))\n","        model.add(Reshape((4, 4, 1024)))\n","        #model.add(UpSampling2D())\n","\n","        #8,8\n","        model.add(Conv2DTranspose(512, kernel_size=4,strides=(2,2), padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.30))\n","        \n","\n","        16,16\n","        model.add(Conv2DTranspose(256, kernel_size=4,strides=(2,2), padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","        \n","\n","        #32,32\n","        model.add(Conv2DTranspose(128, kernel_size=4,strides=(2,2), padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.25))\n","\n","        #64,64\n","        model.add(Conv2DTranspose(64, kernel_size=4,strides=(2,2), padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.20))\n","\n","        #128,128\n","        model.add(Conv2DTranspose(64, kernel_size=4,strides=(2,2), padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.20))\n","        \n","        #256,256\n","        model.add(Conv2DTranspose(self.channels, kernel_size=4,strides=(2,2), padding=\"same\", activation=\"tanh\", name=\"generator_output\"))\n","\n","        model.summary()\n","\n","        noise = Input(shape=(self.latent_dim,))\n","        img = model(noise)\n","\n","        return Model(noise, img, name=\"generator\")\n","\n","    def build_discriminator(self):\n","\n","        model = Sequential()\n","\n","        model.add(Conv2D(64, kernel_size=4, strides=2, input_shape=self.img_shape, padding=\"same\"))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.35))\n","\n","        model.add(Conv2D(64, kernel_size=4, strides=2, input_shape=self.img_shape, padding=\"same\"))\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.35))\n","\n","        model.add(Conv2D(128, kernel_size=4, strides=2, padding=\"same\"))\n","        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.35))\n","\n","        model.add(Conv2D(256, kernel_size=4, strides=2, padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.35))\n","\n","        model.add(Conv2D(512, kernel_size=4, strides=2, padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.35))\n","        \n","        \n","        model.add(Conv2D(1024, kernel_size=5, strides=1, padding=\"same\"))\n","        model.add(BatchNormalization())\n","        model.add(LeakyReLU(alpha=0.2))\n","        model.add(Dropout(0.30))\n","        \n","        \n","        model.add(Flatten())\n","\n","        #model.add(Dense(32, activation='relu'))\n","        model.add(Dense(1, activation='sigmoid'))\n","\n","        model.summary()\n","\n","        \n","        img = Input(shape=self.img_shape)\n","        validity = model(img)\n","\n","        discrim = Model(img, validity)\n","\n","        \n","\n","        discrim.compile(loss='binary_crossentropy',\n","            optimizer=self.optimizer,\n","            metrics=['accuracy'])\n","\n","        return discrim\n","\n","    \n","    def normalize_images(self,im):\n","        return ((im + 1)/2.0)\n","\n","\n","    def train(self, X_train, epochs, batch_size=128, save_interval=100,batch_sample=5):\n","\n","        # Adversarial ground truths\n","\n","\n","        #fixed noise points for using train images\n","\n","        fixed_noise = np.random.normal(0, 1, ( 9 , self.latent_dim)) # generate 9 points\n","        train_imgs = []\n","\n","        for epoch in range(epochs):\n","            \n","            #noisy labels for improved training\n","            #check if this has an effect on disc accuracy?\n","            valid = np.ones((batch_size, 1)) - np.random.uniform(low=0.0, high=0.1, size=(batch_size, 1))\n","            fake = np.zeros((batch_size, 1)) + np.random.uniform(low=0.0, high=0.1, size=(batch_size, 1))\n","            \n","            # ---------------------\n","            #  Train Discriminator\n","            # ---------------------\n","\n","            # Select a random half of images\n","            idx = np.random.randint(0, X_train.shape[0], batch_size)\n","            imgs = X_train[idx]\n","\n","            # Sample noise and generate a batch of new images\n","            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n","            gen_imgs = self.generator.predict(noise)\n","\n","            # Train the discriminator (real classified as ones and generated as zeros)\n","            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n","            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            # ---------------------\n","            #  Train Generator\n","            # ---------------------\n","\n","            # Train the generator (wants discriminator to mistake images as real)\n","            g_loss = self.combined.train_on_batch(noise, valid)\n","\n","            # Plot the progress\n","            if epoch % 10 == 0:\n","                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n","\n","            if epoch % batch_sample == 0:\n","                #work on training animation\n","                epoch_imgs = self.generator.predict(fixed_noise)\n","                train_imgs.append(self.normalize_images(epoch_imgs)) \n","\n","            # If at save interval => save generated image samples\n","            if epoch % save_interval == 0:\n","                self.save_imgs( \"images/{}_{:05d}.png\".format(self.name,epoch) )\n","                # self.combined.save_weights(\"combined_weights ({}).h5\".format(self.name)) # https://github.com/keras-team/keras/issues/10949\n","                self.generator.save(\"generator ({}).h5\".format(self.name))\n","                self.discriminator.save(\"discriminator ({}).h5\".format(self.name))\n","\n","\n","        return train_imgs\n","\n","    def save_models(self):\n","        self.generator.save(\"generator ({}).h5\".format(self.name))\n","        self.discriminator.save(\"discriminator ({}).h5\".format(self.name))\n","\n","    def save_imgs(self, name=''):\n","        r, c = 4, 4\n","        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n","\n","        # replace the first two latent variables with known values\n","        #for i in range(r):\n","        #    for j in range(c):\n","        #        noise[4*i+j][0] = i/(r-1)-0.5\n","        #        noise[4*i+j][1] = j/(c-1)-0.5\n","\n","        gen_imgs = self.generator.predict(noise)\n","        gen_imgs = self.normalize_images(gen_imgs)\n","\n","        fig, axs = plt.subplots(r, c, figsize=(6.72,6.72))\n","        plt.subplots_adjust(left=0.05,bottom=0.05,right=0.95,top=0.95, wspace=0.2, hspace=0.2)\n","\n","        cnt = 0\n","        for i in range(r):\n","            for j in range(c):\n","                axs[i,j].imshow(gen_imgs[cnt])\n","                axs[i,j].axis('off')\n","                cnt += 1\n","\n","        if name:\n","            fig.savefig(name, facecolor='white' )\n","        else: \n","            fig.savefig('{}.png'.format(self.name), facecolor='black' )\n","\n","        plt.close()\n","    \n","\n","def export_model(saver, model, model_name, input_node_names, output_node_name):\n","    from tensorflow.python.tools import freeze_graph\n","    from tensorflow.python.tools import optimize_for_inference_lib\n","    \n","    if not os.path.exists('out'):\n","        os.mkdir('out')\n","\n","    tf.train.write_graph(K.get_session().graph_def, 'out', model_name + '_graph.pbtxt')\n","\n","    saver.save(K.get_session(), 'out/' + model_name + '.chkp')\n","\n","    freeze_graph.freeze_graph('out/' + model_name + '_graph.pbtxt', None, False,\n","                              'out/' + model_name + '.chkp', output_node_name,\n","                              \"save/restore_all\", \"save/Const:0\",\n","                              'out/frozen_' + model_name + '.bytes', True, \"\")\n","\n","    input_graph_def = tf.GraphDef()\n","    with tf.gfile.Open('out/frozen_' + model_name + '.bytes', \"rb\") as f:\n","        input_graph_def.ParseFromString(f.read())\n","\n","    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n","            input_graph_def, input_node_names, [output_node_name],\n","            tf.float32.as_datatype_enum)\n","\n","    with tf.gfile.FastGFile('out/opt_' + model_name + '.bytes', \"wb\") as f:\n","        f.write(output_graph_def.SerializeToString())\n","\n","    print(\"graph saved!\")\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"niTJ4heF8sKv","colab_type":"code","outputId":"8f51ff49-8395-4a2a-cbb3-8f7d58c6428b","executionInfo":{"status":"ok","timestamp":1569325604369,"user_tz":-120,"elapsed":26105,"user":{"displayName":"Antoine T","photoUrl":"","userId":"04821810718843294128"}},"colab":{"base_uri":"https://localhost:8080/","height":776}},"source":["#load dataset\n","import os\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","\n","os.chdir(\"/content/drive/My Drive/GAN/Shipibo GAN/\")\n","!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"," animation\n"," anim_dcgan_128.mp4\n"," anim_grid_2.mp4\n"," anim_grid_3.mp4\n"," anim_grid_dcgan_128.mp4\n"," anim_grid_dcgan_64_2.mp4\n"," anim_grid_dcgan_64.mp4\n"," anim_grid.mp4\n","'Attempts and results.gdoc'\n","'Copy of copy of DCGAN artificial art from github 64.ipynb'\n","'copy of DCGAN artificial art from github.ipynb'\n","'DCGAN 128.ipynb'\n","'DCGAN 256.ipynb'\n","'discriminator (shipibo_128).h5'\n","'discriminator (shipibo_64_black).h5'\n","'discriminator (shipibo_64).h5'\n","'discriminator (shipibo_64_white).h5'\n","'discriminator (shipibo).h5'\n","'generator (shipibo_128).h5'\n","'generator (shipibo_64_black).h5'\n","'generator (shipibo_64).h5'\n","'generator (shipibo_64_white).h5'\n","'generator (shipibo).h5'\n"," imgs_128\n"," imgs_32\n"," imgs_64\n"," preproc\n","'Shipibo GAN 32 V2.ipynb'\n","'Shipibo GAN 64 V2.ipynb'\n"," test_gif_128.mp4\n"," test_gif_2.mp4\n"," test_gif_64.mp4\n"," test_gif_black_64.mp4\n"," test_gif_white_64.mp4\n"," train_grid.mp4\n"," V1\n","'W DCGAN 64.ipynb'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"--tMDwmzwvtk","colab_type":"text"},"source":["https://github.com/googlecolab/colabtools/issues/287#issuecomment-517446876\n","\n","measures when vm wont synch with drive\n","\n"]},{"cell_type":"code","metadata":{"id":"E9pOdnSLwZP7","colab_type":"code","colab":{}},"source":["#drive.flush_and_unmount()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2GotuNDo-Hyj","colab_type":"code","colab":{}},"source":["\n","!mkdir imgs_256\n","!unzip -q preproc/256/0_compressed_256.zip -d imgs_256/\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BaU1AGJRBkkJ","colab_type":"code","colab":{}},"source":["!mkdir images"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mbp2X380-Oq4","colab_type":"code","outputId":"16064278-7a9e-4735-ff15-395c446d50ba","executionInfo":{"status":"ok","timestamp":1569325891154,"user_tz":-120,"elapsed":312776,"user":{"displayName":"Antoine T","photoUrl":"","userId":"04821810718843294128"}},"colab":{"base_uri":"https://localhost:8080/","height":334}},"source":["from keras.preprocessing.image import load_img,img_to_array\n","from PIL import Image\n","\n","def load_images(path,size=256,final_size=0):\n","    \"\"\" loads images into a numpy array and returns them\n","    \"\"\"\n","    img_paths = [path + f  for f in os.listdir(path)]\n","\n","    print(len(img_paths))\n","\n","    if final_size== 0:\n","        final_size = len(img_paths)\n","\n","    dataset_shape = (final_size ,size,size,3)\n","\n","    data = np.ndarray(shape=dataset_shape)\n","\n","    for i,f in enumerate(img_paths):\n","        with Image.open(f) as im:\n","            \n","            im = np.array(im)\n","            \n","            if i % 500 == 0:\n","                print(\"loaded \", i,\"images\" )\n","\n","            data[i] = im.astype(int)\n","\n","            if (i+1) % final_size == 0:\n","\n","                return data\n","            \n","    print(data.shape)\n","\n","    return data\n","\n","data = load_images(\"imgs_256/\",final_size=8000)\n","\n","#normalize data to tanh dim\n","data  = (data - 127.5) / 127.5"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["14700\n","loaded  0 images\n","loaded  500 images\n","loaded  1000 images\n","loaded  1500 images\n","loaded  2000 images\n","loaded  2500 images\n","loaded  3000 images\n","loaded  3500 images\n","loaded  4000 images\n","loaded  4500 images\n","loaded  5000 images\n","loaded  5500 images\n","loaded  6000 images\n","loaded  6500 images\n","loaded  7000 images\n","loaded  7500 images\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tmzDmYTHrXcK","colab_type":"code","colab":{}},"source":["from matplotlib import animation\n","\n","def animate_single_frame(train_imgs,index=0):\n","    \n","\n","    ims = []\n","    fig = plt.figure()\n","    ax = fig.add_axes([0,0,1,1], frameon=False, aspect=1)\n","    ax.set_xticks([]); ax.set_yticks([])\n","\n","    for train_ims in train_imgs:\n","\n","        \n","\n","        im_1 = plt.imshow(train_ims[0,:,:],vmin=0, vmax=1, animated=True) #add first image for test\n","        \n","        ims.append([im_1])\n","        #plt.pause(0.1) \n","\n","    mp4_writer =  animation.writers['ffmpeg']\n","    writer = mp4_writer(fps=24, metadata=dict(artist='Me'), bitrate=1800)\n","\n","\n","    anim = animation.ArtistAnimation(fig,ims)\n","    anim.save(\"test_gif_256.mp4\", writer= writer)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uBZcqCZeCjOU","colab_type":"code","outputId":"b9a101de-855e-4c1e-9acb-3d258517776c","executionInfo":{"status":"ok","timestamp":1569325896475,"user_tz":-120,"elapsed":318069,"user":{"displayName":"Antoine T","photoUrl":"","userId":"04821810718843294128"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["preload_model = False\n","\n","dcgan =  DCGAN(latent_dim=128) #create class\n","\n","if preload_model:\n","    dcgan.load_weights(generator_file=\"generator (shipibo_256).h5\",discriminator_file=\"discriminator (shipibo_256).h5\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d (Conv2D)              (None, 128, 128, 64)      3136      \n","_________________________________________________________________\n","leaky_re_lu (LeakyReLU)      (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","dropout (Dropout)            (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","conv2d_1 (Conv2D)            (None, 64, 64, 64)        65600     \n","_________________________________________________________________\n","leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","conv2d_2 (Conv2D)            (None, 32, 32, 128)       131200    \n","_________________________________________________________________\n","zero_padding2d (ZeroPadding2 (None, 33, 33, 128)       0         \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 33, 33, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_2 (LeakyReLU)    (None, 33, 33, 128)       0         \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 33, 33, 128)       0         \n","_________________________________________________________________\n","conv2d_3 (Conv2D)            (None, 17, 17, 256)       524544    \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 17, 17, 256)       1024      \n","_________________________________________________________________\n","leaky_re_lu_3 (LeakyReLU)    (None, 17, 17, 256)       0         \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 17, 17, 256)       0         \n","_________________________________________________________________\n","conv2d_4 (Conv2D)            (None, 9, 9, 512)         2097664   \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 9, 9, 512)         2048      \n","_________________________________________________________________\n","leaky_re_lu_4 (LeakyReLU)    (None, 9, 9, 512)         0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 9, 9, 512)         0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 9, 9, 1024)        13108224  \n","_________________________________________________________________\n","batch_normalization_3 (Batch (None, 9, 9, 1024)        4096      \n","_________________________________________________________________\n","leaky_re_lu_5 (LeakyReLU)    (None, 9, 9, 1024)        0         \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 9, 9, 1024)        0         \n","_________________________________________________________________\n","flatten (Flatten)            (None, 82944)             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 1)                 82945     \n","=================================================================\n","Total params: 16,020,993\n","Trainable params: 16,017,153\n","Non-trainable params: 3,840\n","_________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Model: \"sequential_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","generator_input (Dense)      (None, 16384)             2113536   \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 16384)             0         \n","_________________________________________________________________\n","reshape (Reshape)            (None, 4, 4, 1024)        0         \n","_________________________________________________________________\n","conv2d_transpose (Conv2DTran (None, 8, 8, 512)         8389120   \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 8, 8, 512)         2048      \n","_________________________________________________________________\n","leaky_re_lu_6 (LeakyReLU)    (None, 8, 8, 512)         0         \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 8, 8, 512)         0         \n","_________________________________________________________________\n","conv2d_transpose_1 (Conv2DTr (None, 16, 16, 256)       2097408   \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 16, 16, 256)       1024      \n","_________________________________________________________________\n","leaky_re_lu_7 (LeakyReLU)    (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","dropout_8 (Dropout)          (None, 16, 16, 256)       0         \n","_________________________________________________________________\n","conv2d_transpose_2 (Conv2DTr (None, 32, 32, 128)       524416    \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 32, 32, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_8 (LeakyReLU)    (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","dropout_9 (Dropout)          (None, 32, 32, 128)       0         \n","_________________________________________________________________\n","conv2d_transpose_3 (Conv2DTr (None, 64, 64, 64)        131136    \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 64, 64, 64)        256       \n","_________________________________________________________________\n","leaky_re_lu_9 (LeakyReLU)    (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","dropout_10 (Dropout)         (None, 64, 64, 64)        0         \n","_________________________________________________________________\n","conv2d_transpose_4 (Conv2DTr (None, 128, 128, 64)      65600     \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 128, 128, 64)      256       \n","_________________________________________________________________\n","leaky_re_lu_10 (LeakyReLU)   (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","dropout_11 (Dropout)         (None, 128, 128, 64)      0         \n","_________________________________________________________________\n","generator_output (Conv2DTran (None, 256, 256, 3)       3075      \n","=================================================================\n","Total params: 13,328,387\n","Trainable params: 13,326,339\n","Non-trainable params: 2,048\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kg_unsMP-pRm","colab_type":"code","outputId":"e64752d6-1f90-469d-f9df-6faf0d9cee50","executionInfo":{"status":"error","timestamp":1569002866934,"user_tz":-120,"elapsed":5089769,"user":{"displayName":"Antoine T","photoUrl":"","userId":"04821810718843294128"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["\n","\n","train_imgs = dcgan.train(data,epochs=5000,batch_size=32,batch_sample=3)\n","\n","animate_single_frame(train_imgs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","WARNING:tensorflow:Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","0 [D loss: 7.179680, acc.: 0.00%] [G loss: 0.194808]\n","10 [D loss: 0.751172, acc.: 0.00%] [G loss: 0.265010]\n","20 [D loss: 0.686329, acc.: 0.00%] [G loss: 0.305598]\n","30 [D loss: 0.349801, acc.: 0.00%] [G loss: 0.318055]\n","40 [D loss: 0.492478, acc.: 0.00%] [G loss: 0.398901]\n","50 [D loss: 0.255437, acc.: 0.00%] [G loss: 0.260598]\n","60 [D loss: 0.410745, acc.: 0.00%] [G loss: 0.286036]\n","70 [D loss: 0.392418, acc.: 0.00%] [G loss: 0.457610]\n","80 [D loss: 0.665535, acc.: 0.00%] [G loss: 1.395987]\n","90 [D loss: 1.601353, acc.: 0.00%] [G loss: 11.179845]\n","100 [D loss: 1.313662, acc.: 0.00%] [G loss: 0.417968]\n","110 [D loss: 2.400643, acc.: 0.00%] [G loss: 26.051277]\n","120 [D loss: 5.339884, acc.: 0.00%] [G loss: 13.554980]\n","130 [D loss: 1.090877, acc.: 0.00%] [G loss: 0.837574]\n","140 [D loss: 0.580302, acc.: 0.00%] [G loss: 0.922943]\n","150 [D loss: 0.284166, acc.: 0.00%] [G loss: 0.529595]\n","160 [D loss: 0.411461, acc.: 0.00%] [G loss: 0.323962]\n","170 [D loss: 0.425534, acc.: 0.00%] [G loss: 1.308294]\n","180 [D loss: 0.360347, acc.: 0.00%] [G loss: 1.184560]\n","190 [D loss: 0.574426, acc.: 0.00%] [G loss: 0.305576]\n","200 [D loss: 0.321612, acc.: 0.00%] [G loss: 0.531664]\n","210 [D loss: 1.366364, acc.: 0.00%] [G loss: 0.342388]\n","220 [D loss: 0.383118, acc.: 0.00%] [G loss: 6.468819]\n","230 [D loss: 0.265409, acc.: 0.00%] [G loss: 8.002779]\n","240 [D loss: 0.332926, acc.: 0.00%] [G loss: 12.158621]\n","250 [D loss: 0.288326, acc.: 0.00%] [G loss: 2.577265]\n","260 [D loss: 0.727470, acc.: 0.00%] [G loss: 14.448792]\n","270 [D loss: 1.566119, acc.: 0.00%] [G loss: 37.534916]\n","280 [D loss: 0.383893, acc.: 0.00%] [G loss: 1.552012]\n","290 [D loss: 0.225616, acc.: 0.00%] [G loss: 0.215961]\n","300 [D loss: 0.506920, acc.: 0.00%] [G loss: 0.515817]\n","310 [D loss: 0.489731, acc.: 0.00%] [G loss: 0.378200]\n","320 [D loss: 0.373371, acc.: 0.00%] [G loss: 0.290795]\n","330 [D loss: 0.278151, acc.: 0.00%] [G loss: 0.264522]\n","340 [D loss: 0.229341, acc.: 0.00%] [G loss: 0.240632]\n","350 [D loss: 0.246763, acc.: 0.00%] [G loss: 0.291617]\n","360 [D loss: 0.266210, acc.: 0.00%] [G loss: 0.299575]\n","370 [D loss: 0.235473, acc.: 0.00%] [G loss: 0.210515]\n","380 [D loss: 0.208410, acc.: 0.00%] [G loss: 0.205096]\n","390 [D loss: 0.558544, acc.: 0.00%] [G loss: 7.551714]\n","400 [D loss: 0.291067, acc.: 0.00%] [G loss: 0.268890]\n","410 [D loss: 0.300308, acc.: 0.00%] [G loss: 0.240789]\n","420 [D loss: 0.906406, acc.: 0.00%] [G loss: 6.076002]\n","430 [D loss: 1.569588, acc.: 0.00%] [G loss: 1.793956]\n","440 [D loss: 0.362634, acc.: 0.00%] [G loss: 0.303991]\n","450 [D loss: 0.532869, acc.: 0.00%] [G loss: 3.225463]\n","460 [D loss: 0.618833, acc.: 0.00%] [G loss: 1.642238]\n","470 [D loss: 1.323458, acc.: 0.00%] [G loss: 10.373164]\n","480 [D loss: 0.807706, acc.: 0.00%] [G loss: 1.266701]\n","490 [D loss: 1.153868, acc.: 0.00%] [G loss: 7.429852]\n","500 [D loss: 0.256330, acc.: 0.00%] [G loss: 0.721575]\n","510 [D loss: 0.313015, acc.: 0.00%] [G loss: 3.644464]\n","520 [D loss: 0.545608, acc.: 0.00%] [G loss: 1.541435]\n","530 [D loss: 0.726298, acc.: 0.00%] [G loss: 2.867634]\n","540 [D loss: 0.497328, acc.: 0.00%] [G loss: 1.910729]\n","550 [D loss: 0.459305, acc.: 0.00%] [G loss: 1.981951]\n","560 [D loss: 0.322753, acc.: 0.00%] [G loss: 0.512412]\n","570 [D loss: 0.325853, acc.: 0.00%] [G loss: 0.315688]\n","580 [D loss: 0.563647, acc.: 0.00%] [G loss: 0.637083]\n","590 [D loss: 1.121151, acc.: 0.00%] [G loss: 3.157738]\n","600 [D loss: 0.308216, acc.: 0.00%] [G loss: 6.609990]\n","610 [D loss: 0.264787, acc.: 0.00%] [G loss: 3.363131]\n","620 [D loss: 0.402162, acc.: 0.00%] [G loss: 2.409673]\n","630 [D loss: 0.199086, acc.: 0.00%] [G loss: 0.208202]\n","640 [D loss: 0.211199, acc.: 0.00%] [G loss: 0.290640]\n","650 [D loss: 0.240163, acc.: 0.00%] [G loss: 0.212732]\n","660 [D loss: 0.195941, acc.: 0.00%] [G loss: 0.348147]\n","670 [D loss: 0.209936, acc.: 0.00%] [G loss: 0.224765]\n","680 [D loss: 0.208381, acc.: 0.00%] [G loss: 0.210009]\n","690 [D loss: 0.358894, acc.: 0.00%] [G loss: 2.934335]\n","700 [D loss: 0.500948, acc.: 0.00%] [G loss: 5.638346]\n","710 [D loss: 0.907092, acc.: 0.00%] [G loss: 3.802154]\n","720 [D loss: 1.127655, acc.: 0.00%] [G loss: 3.458242]\n","730 [D loss: 0.775252, acc.: 0.00%] [G loss: 1.791041]\n","740 [D loss: 0.346801, acc.: 0.00%] [G loss: 1.251304]\n","750 [D loss: 0.286219, acc.: 0.00%] [G loss: 0.966544]\n","760 [D loss: 0.443342, acc.: 0.00%] [G loss: 0.307918]\n","770 [D loss: 0.302855, acc.: 0.00%] [G loss: 0.310967]\n","780 [D loss: 0.307020, acc.: 0.00%] [G loss: 0.327186]\n","790 [D loss: 0.229320, acc.: 0.00%] [G loss: 0.237618]\n","800 [D loss: 0.232254, acc.: 0.00%] [G loss: 0.266663]\n","810 [D loss: 0.246036, acc.: 0.00%] [G loss: 0.407748]\n","820 [D loss: 0.261808, acc.: 0.00%] [G loss: 0.602470]\n","830 [D loss: 0.334842, acc.: 0.00%] [G loss: 1.803590]\n","840 [D loss: 1.318234, acc.: 0.00%] [G loss: 6.225465]\n","850 [D loss: 0.420970, acc.: 0.00%] [G loss: 6.446391]\n","860 [D loss: 0.462356, acc.: 0.00%] [G loss: 10.579033]\n","870 [D loss: 0.326288, acc.: 0.00%] [G loss: 0.629467]\n","880 [D loss: 0.395264, acc.: 0.00%] [G loss: 0.226505]\n","890 [D loss: 0.255302, acc.: 0.00%] [G loss: 0.542224]\n","900 [D loss: 0.222465, acc.: 0.00%] [G loss: 0.232878]\n","910 [D loss: 0.236783, acc.: 0.00%] [G loss: 0.234594]\n","920 [D loss: 0.230385, acc.: 0.00%] [G loss: 0.230466]\n","930 [D loss: 0.231427, acc.: 0.00%] [G loss: 0.227171]\n","940 [D loss: 0.218679, acc.: 0.00%] [G loss: 0.313667]\n","950 [D loss: 0.250491, acc.: 0.00%] [G loss: 0.246707]\n","960 [D loss: 0.226842, acc.: 0.00%] [G loss: 0.240843]\n","970 [D loss: 0.219885, acc.: 0.00%] [G loss: 0.231140]\n","980 [D loss: 0.240171, acc.: 0.00%] [G loss: 0.231626]\n","990 [D loss: 0.861051, acc.: 0.00%] [G loss: 2.044454]\n","1000 [D loss: 0.355806, acc.: 0.00%] [G loss: 3.057338]\n","1010 [D loss: 1.928378, acc.: 0.00%] [G loss: 2.840921]\n","1020 [D loss: 0.429693, acc.: 0.00%] [G loss: 2.064968]\n","1030 [D loss: 0.244863, acc.: 0.00%] [G loss: 0.230385]\n","1040 [D loss: 0.262973, acc.: 0.00%] [G loss: 0.208117]\n","1050 [D loss: 0.227037, acc.: 0.00%] [G loss: 0.233237]\n","1060 [D loss: 0.664876, acc.: 0.00%] [G loss: 2.174396]\n","1070 [D loss: 0.248356, acc.: 0.00%] [G loss: 3.662818]\n","1080 [D loss: 0.253003, acc.: 0.00%] [G loss: 1.232824]\n","1090 [D loss: 0.522961, acc.: 0.00%] [G loss: 2.454693]\n","1100 [D loss: 0.277320, acc.: 0.00%] [G loss: 1.456212]\n","1110 [D loss: 0.244079, acc.: 0.00%] [G loss: 1.323136]\n","1120 [D loss: 0.729576, acc.: 0.00%] [G loss: 5.964791]\n","1130 [D loss: 0.303871, acc.: 0.00%] [G loss: 0.540276]\n","1140 [D loss: 0.365079, acc.: 0.00%] [G loss: 0.334033]\n","1150 [D loss: 0.570384, acc.: 0.00%] [G loss: 0.247950]\n","1160 [D loss: 0.259958, acc.: 0.00%] [G loss: 0.218820]\n","1170 [D loss: 0.278598, acc.: 0.00%] [G loss: 0.435981]\n","1180 [D loss: 0.205942, acc.: 0.00%] [G loss: 0.227093]\n","1190 [D loss: 0.217582, acc.: 0.00%] [G loss: 0.265132]\n","1200 [D loss: 0.313802, acc.: 0.00%] [G loss: 0.258851]\n","1210 [D loss: 0.279761, acc.: 0.00%] [G loss: 0.615924]\n","1220 [D loss: 0.264717, acc.: 0.00%] [G loss: 0.299595]\n","1230 [D loss: 0.236585, acc.: 0.00%] [G loss: 0.288720]\n","1240 [D loss: 0.514653, acc.: 0.00%] [G loss: 5.719174]\n","1250 [D loss: 0.557913, acc.: 0.00%] [G loss: 1.510647]\n","1260 [D loss: 0.339369, acc.: 0.00%] [G loss: 0.828783]\n","1270 [D loss: 0.212468, acc.: 0.00%] [G loss: 0.363578]\n","1280 [D loss: 0.228589, acc.: 0.00%] [G loss: 0.268758]\n","1290 [D loss: 0.272085, acc.: 0.00%] [G loss: 0.607779]\n","1300 [D loss: 0.379711, acc.: 0.00%] [G loss: 0.324112]\n","1310 [D loss: 0.303899, acc.: 0.00%] [G loss: 0.266754]\n","1320 [D loss: 0.248982, acc.: 0.00%] [G loss: 0.244554]\n","1330 [D loss: 0.307959, acc.: 0.00%] [G loss: 0.267802]\n","1340 [D loss: 0.295022, acc.: 0.00%] [G loss: 0.331478]\n","1350 [D loss: 0.336761, acc.: 0.00%] [G loss: 0.599838]\n","1360 [D loss: 0.266215, acc.: 0.00%] [G loss: 0.265586]\n","1370 [D loss: 0.246814, acc.: 0.00%] [G loss: 0.252615]\n","1380 [D loss: 0.224432, acc.: 0.00%] [G loss: 0.240396]\n","1390 [D loss: 0.238805, acc.: 0.00%] [G loss: 0.320956]\n","1400 [D loss: 0.250056, acc.: 0.00%] [G loss: 0.592403]\n","1410 [D loss: 0.261176, acc.: 0.00%] [G loss: 0.248997]\n","1420 [D loss: 0.259956, acc.: 0.00%] [G loss: 0.234966]\n","1430 [D loss: 0.243253, acc.: 0.00%] [G loss: 0.240691]\n","1440 [D loss: 0.357000, acc.: 0.00%] [G loss: 0.236947]\n","1450 [D loss: 0.249535, acc.: 0.00%] [G loss: 0.225560]\n","1460 [D loss: 0.219542, acc.: 0.00%] [G loss: 0.193223]\n","1470 [D loss: 0.205573, acc.: 0.00%] [G loss: 0.210639]\n","1480 [D loss: 0.225629, acc.: 0.00%] [G loss: 0.242964]\n","1490 [D loss: 0.201790, acc.: 0.00%] [G loss: 0.202023]\n","1500 [D loss: 0.198417, acc.: 0.00%] [G loss: 0.206566]\n","1510 [D loss: 0.208273, acc.: 0.00%] [G loss: 0.237600]\n","1520 [D loss: 0.231004, acc.: 0.00%] [G loss: 0.227553]\n","1530 [D loss: 3.632982, acc.: 0.00%] [G loss: 18.052334]\n","1540 [D loss: 0.338841, acc.: 0.00%] [G loss: 0.386575]\n","1550 [D loss: 0.215023, acc.: 0.00%] [G loss: 0.282821]\n","1560 [D loss: 0.359792, acc.: 0.00%] [G loss: 0.946727]\n","1570 [D loss: 0.292538, acc.: 0.00%] [G loss: 0.506611]\n","1580 [D loss: 0.447561, acc.: 0.00%] [G loss: 0.265981]\n","1590 [D loss: 0.206862, acc.: 0.00%] [G loss: 0.188926]\n","1600 [D loss: 0.231138, acc.: 0.00%] [G loss: 0.268130]\n","1610 [D loss: 0.516907, acc.: 0.00%] [G loss: 2.633097]\n","1620 [D loss: 0.374191, acc.: 0.00%] [G loss: 1.548943]\n","1630 [D loss: 0.284748, acc.: 0.00%] [G loss: 0.250899]\n","1640 [D loss: 0.408513, acc.: 0.00%] [G loss: 0.543128]\n","1650 [D loss: 0.221796, acc.: 0.00%] [G loss: 0.219903]\n","1660 [D loss: 0.294936, acc.: 0.00%] [G loss: 0.219081]\n","1670 [D loss: 0.250098, acc.: 0.00%] [G loss: 0.261946]\n","1680 [D loss: 0.283597, acc.: 0.00%] [G loss: 0.242217]\n","1690 [D loss: 0.232368, acc.: 0.00%] [G loss: 0.226203]\n","1700 [D loss: 4.098530, acc.: 0.00%] [G loss: 0.274556]\n","1710 [D loss: 0.267026, acc.: 0.00%] [G loss: 0.486720]\n","1720 [D loss: 0.238516, acc.: 0.00%] [G loss: 0.238066]\n","1730 [D loss: 0.214319, acc.: 0.00%] [G loss: 0.267979]\n","1740 [D loss: 0.239405, acc.: 0.00%] [G loss: 0.299015]\n","1750 [D loss: 0.230691, acc.: 0.00%] [G loss: 0.223517]\n","1760 [D loss: 0.342645, acc.: 0.00%] [G loss: 0.247445]\n","1770 [D loss: 0.228703, acc.: 0.00%] [G loss: 0.233415]\n","1780 [D loss: 0.267280, acc.: 0.00%] [G loss: 0.439208]\n","1790 [D loss: 0.503521, acc.: 0.00%] [G loss: 0.328028]\n","1800 [D loss: 0.234252, acc.: 0.00%] [G loss: 0.191111]\n","1810 [D loss: 0.266914, acc.: 0.00%] [G loss: 0.511980]\n","1820 [D loss: 0.248560, acc.: 0.00%] [G loss: 0.214758]\n","1830 [D loss: 0.197710, acc.: 0.00%] [G loss: 0.206986]\n","1840 [D loss: 0.235069, acc.: 0.00%] [G loss: 0.293319]\n","1850 [D loss: 0.620313, acc.: 0.00%] [G loss: 0.387109]\n","1860 [D loss: 0.221649, acc.: 0.00%] [G loss: 0.212011]\n","1870 [D loss: 0.215153, acc.: 0.00%] [G loss: 0.227430]\n","1880 [D loss: 0.223316, acc.: 0.00%] [G loss: 0.179124]\n","1890 [D loss: 0.204211, acc.: 0.00%] [G loss: 0.205943]\n","1900 [D loss: 0.232797, acc.: 0.00%] [G loss: 0.266119]\n","1910 [D loss: 0.267518, acc.: 0.00%] [G loss: 0.289161]\n","1920 [D loss: 0.244887, acc.: 0.00%] [G loss: 0.233799]\n","1930 [D loss: 0.242327, acc.: 0.00%] [G loss: 0.261284]\n","1940 [D loss: 0.204096, acc.: 0.00%] [G loss: 0.226542]\n","1950 [D loss: 1.701729, acc.: 0.00%] [G loss: 1.902276]\n","1960 [D loss: 0.277122, acc.: 0.00%] [G loss: 0.322833]\n","1970 [D loss: 0.226206, acc.: 0.00%] [G loss: 0.224946]\n","1980 [D loss: 0.337822, acc.: 0.00%] [G loss: 0.387107]\n","1990 [D loss: 0.245624, acc.: 0.00%] [G loss: 0.232815]\n","2000 [D loss: 0.666078, acc.: 0.00%] [G loss: 0.582750]\n","2010 [D loss: 0.434836, acc.: 0.00%] [G loss: 0.375039]\n","2020 [D loss: 0.262720, acc.: 0.00%] [G loss: 0.261086]\n","2030 [D loss: 0.260426, acc.: 0.00%] [G loss: 0.226853]\n","2040 [D loss: 0.250755, acc.: 0.00%] [G loss: 0.224180]\n","2050 [D loss: 0.224416, acc.: 0.00%] [G loss: 0.205481]\n","2060 [D loss: 0.280033, acc.: 0.00%] [G loss: 0.228067]\n","2070 [D loss: 0.234447, acc.: 0.00%] [G loss: 0.244505]\n","2080 [D loss: 0.224946, acc.: 0.00%] [G loss: 0.211773]\n","2090 [D loss: 0.344842, acc.: 0.00%] [G loss: 0.539784]\n","2100 [D loss: 0.241941, acc.: 0.00%] [G loss: 0.228622]\n","2110 [D loss: 0.224010, acc.: 0.00%] [G loss: 0.276664]\n","2120 [D loss: 0.245566, acc.: 0.00%] [G loss: 0.200771]\n","2130 [D loss: 0.254758, acc.: 0.00%] [G loss: 0.245218]\n","2140 [D loss: 0.191514, acc.: 0.00%] [G loss: 0.242306]\n","2150 [D loss: 1.668228, acc.: 0.00%] [G loss: 0.334208]\n","2160 [D loss: 0.229335, acc.: 0.00%] [G loss: 0.228517]\n","2170 [D loss: 0.247713, acc.: 0.00%] [G loss: 0.201495]\n","2180 [D loss: 0.225103, acc.: 0.00%] [G loss: 0.227099]\n","2190 [D loss: 0.212295, acc.: 0.00%] [G loss: 0.289657]\n","2200 [D loss: 0.216652, acc.: 0.00%] [G loss: 0.226594]\n","2210 [D loss: 0.231324, acc.: 0.00%] [G loss: 0.213144]\n","2220 [D loss: 0.254346, acc.: 0.00%] [G loss: 1.220517]\n","2230 [D loss: 0.247612, acc.: 0.00%] [G loss: 0.254315]\n","2240 [D loss: 0.199803, acc.: 0.00%] [G loss: 0.201692]\n","2250 [D loss: 0.233541, acc.: 0.00%] [G loss: 0.217212]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ohKbuY3eBjx0","colab_type":"code","colab":{}},"source":["\n","noise = np.random.normal(0, 1, ( 9 , dcgan.latent_dim)) # generate 9 points\n","imgs = dcgan.normalize_images(dcgan.generator.predict(noise))\n","\n","for i in range(len(noise)):\n","\n","    plt.subplot(3,3, i+1)\n","    plt.imshow(imgs[i])\n","\n","plt.show\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lSGJssMslKSl","colab_type":"code","colab":{}},"source":["\n","\n","# starting point for every image\n","seed_start = np.random.normal(0, 1, (9, dcgan.latent_dim))\n","\n","# these parameters will change every time step\n","latentSpeed = np.random.normal(5, 1, (9, dcgan.latent_dim))\n","vary = np.copy(seed_start)\n","\n","# video settings\n","time = 0\n","fps = 30\n","maxTime = 20 # seconds\n","frameCount = 0\n","\n","anim_imgs = []\n","\n","while (time <= maxTime):\n","\n","    # for each image\n","    for i in range(len(seed_start)): \n","        \n","        # change the latent variables\n","        for j in range(dcgan.latent_dim):\n","            vary[i][j] = seed_start[i][j] + np.sin( 2*np.pi*(time/maxTime) * latentSpeed[i][j] ) \n","\n","    gen_imgs = dcgan.generator.predict(vary)\n","\n","    anim_imgs.append(dcgan.normalize_images(gen_imgs))\n","    \n","\n","    frameCount += 1\n","    time += .70/fps"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"peuHxidJlmKJ","colab_type":"code","colab":{}},"source":["from matplotlib import animation\n","\n","fig,axs = plt.subplots(3,3)\n","\n","print(axs.shape)\n","\n","for i in range(axs.shape[0]):\n","    for j in range(axs.shape[1]):\n","        \n","        axs[i,j].set_xticks([])\n","        axs[i,j].set_yticks([])\n","\n","images = []\n","\n","#iterate ove images\n","for imgs in anim_imgs:\n","    ims = [] \n","    for i,img in enumerate(imgs):\n","        \n","        line = i%3\n","        col = int(i/3)\n","        ims.append(axs[line,col].imshow(img[:,:],vmin=0, vmax=1, animated=True))\n","    \n","    images.append(ims)\n","\n","mp4_writer =  animation.writers['ffmpeg']\n","writer = mp4_writer(fps=24, metadata=dict(artist='Me'), bitrate=1800)\n","\n","anim = animation.ArtistAnimation(fig,images)\n","anim.save(\"anim_grid_dcgan_256.mp4\", writer= writer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"daSw0k1Vx1BO","colab_type":"code","colab":{}},"source":["from IPython.display import HTML\n","from matplotlib import animation\n","\n","ims = []\n","fig = plt.figure() \n","ax = fig.add_axes([0,0,1,1], frameon=False, aspect=1)\n","ax.set_xticks([]); ax.set_yticks([])\n","\n","for train_ims in anim_imgs:\n","\n","    \n","\n","    im_1 = plt.imshow(train_ims[1,:,:],vmin=0, vmax=1, animated=True) #add first image for test\n","    \n","    ims.append([im_1])\n","    #plt.pause(0.1) \n","\n","mp4_writer =  animation.writers['ffmpeg']\n","writer = mp4_writer(fps=24, metadata=dict(artist='Me'), bitrate=1800)\n","\n","fps=30\n","\n","anim = animation.ArtistAnimation(fig,ims,interval=(1/fps)*1000)\n","#anim.save(\"anim_dcgan_256.mp4\", writer= writer)\n","HTML(anim.to_html5_video())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R4KPSnP_Rjxv","colab_type":"code","colab":{}},"source":["from matplotlib import animation\n","from IPython.display import HTML\n","fig,axs = plt.subplots(3,3)\n","\n","print(axs.shape)\n","\n","for i in range(axs.shape[0]):\n","    for j in range(axs.shape[1]):\n","        \n","        axs[i,j].set_xticks([])\n","        axs[i,j].set_yticks([])\n","\n","images = []\n","\n","#iterate ove images\n","for imgs in anim_imgs:\n","    ims = [] \n","    for i,img in enumerate(imgs):\n","        \n","        line = i%3\n","        col = int(i/3)\n","        ims.append(axs[line,col].imshow(img[:,:],vmin=0, vmax=1, animated=True))\n","    \n","    images.append(ims)\n","\n","\n","\n","fps=30\n","\n","anim = animation.ArtistAnimation(fig,images,interval=(1/fps)*1000)\n","HTML(anim.to_html5_video())"],"execution_count":0,"outputs":[]}]}