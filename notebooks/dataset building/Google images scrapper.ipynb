{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## google images web scrapper\n",
    "\n",
    "Little project to scrap images from google. a list of keywords can be specified as a search parameter.\n",
    "\n",
    "this article proved to be usefull as scraping images from google requires some more unconventional methods as they have put in place lots of protection against web scraping:\n",
    "https://medium.com/france-school-of-ai/scraper-un-dataset-depuis-google-images-342f670b9bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:05:50.212941Z",
     "start_time": "2019-07-14T16:05:50.208953Z"
    }
   },
   "outputs": [],
   "source": [
    "import selenium\n",
    "import selenium.webdriver\n",
    "import time\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:05:56.757532Z",
     "start_time": "2019-07-14T16:05:51.351737Z"
    }
   },
   "outputs": [],
   "source": [
    "driver = selenium.webdriver.Chrome(\"/mnt/c/Program Files (x86)/Google/Chrome/Application/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T13:53:29.633056Z",
     "start_time": "2019-07-14T13:53:22.703151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "start_url = \"https://www.google.com/imghp\"\n",
    "js_image_links_selector = \"return Array.from(document.querySelectorAll('.rg_di .rg_meta')).map(el=>JSON.parse(el.textContent).ou);\"\n",
    "js_scrolldown = \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "\n",
    "driver.get(start_url)\n",
    "\n",
    "\n",
    "input_box = driver.find_element_by_class_name(\"gLFyf\")\n",
    "input_box.send_keys(\"shipibo\" + \"\\n\")\n",
    "\n",
    "#we scrolldown once so that we can get more picctures\n",
    "driver.execute_script(js_scrolldown)\n",
    "time.sleep(5)\n",
    "links= driver.execute_script(js_image_links_selector)\n",
    "\n",
    "\n",
    "\n",
    "print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) link collection\n",
    "Instead of going through every single image on google, right clicking and selecting \"open image ina  new tab\" which i'm pretty sur is not even possible even using selenium we will instead run a small javascript function that will return a list of links that will have the same effect as doing the tedious operation mentioned above. \n",
    "\n",
    "This will avoid us from eventually getting our IP address blocked by google after frantiaclly looking at every picture that they have and downloading them like a madman. This also means that we have no need for using a proxy. This wouldve been necessary had we been scraping images from a single provider; but, it is most likely that the pictures that we will be downloading will come from various content managers (cdn) which will not find our downloading activity very suspicious.\n",
    "\n",
    "## the function\n",
    "\n",
    "This function will therfore have a list of terms to search for, all relating to the subject that we wish to scrape images from. For every search term, we will first go down in order to have google load an extra 100 pictures for us (we could repeat this to collect up to 700 images but they will not all be relevant by that stage). after this part, we will execute a small javascript function (idk how it works) that will return all of the proper links from which we can directly download the images from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:06:57.134784Z",
     "start_time": "2019-07-14T16:06:27.784233Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_links(search_terms,driver):\n",
    "    #google images home page\n",
    "    start_url = \"https://www.google.com/imghp\"\n",
    "    #javascript functions that we will use to gather the links\n",
    "    js_image_links_selector = \"return Array.from(document.querySelectorAll('.rg_di .rg_meta')).map(el=>JSON.parse(el.textContent).ou);\"\n",
    "    js_scrolldown = \"window.scrollTo(0, document.body.scrollHeight);\"\n",
    "    \n",
    "    image_link_list = []\n",
    "    \n",
    "    for term in search_terms:\n",
    "    \n",
    "        driver.get(start_url)\n",
    "        #find search bar\n",
    "        input_box = driver.find_element_by_class_name(\"gLFyf\")\n",
    "        input_box.send_keys(term + \"\\n\")\n",
    "        \n",
    "        #we scrolldown once so that we can get more picctures\n",
    "        driver.execute_script(js_scrolldown)\n",
    "        #sleep a random amount of time, we need enough to let google load the rest of the pictures\n",
    "        time.sleep(np.random.randint(2,5))\n",
    "        driver.execute_script(js_scrolldown) #repeat to get 300 links per term searched\n",
    "        time.sleep(np.random.randint(2,5))\n",
    "        \n",
    "        term_links = driver.execute_script(js_image_links_selector)\n",
    "        image_link_list.append(term_links)\n",
    "        \n",
    "        \n",
    "        #sleep again for a random amount of time\n",
    "        time.sleep(np.random.randint(1,4))\n",
    "        \n",
    "    return image_link_list\n",
    "\n",
    "\n",
    "terms_list = [\"shipibo art\", \"shipibo patterns\",\"shipibo tapestry\",\"shipibo textiles\"]\n",
    "img_links = collect_links(terms_list,driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-07-14T16:21:53.054634Z",
     "start_time": "2019-07-14T16:21:53.049663Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "300\n",
      "300\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for links in img_links:\n",
    "    print(len(links))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) downloading the images\n",
    "Now that we have gathered all of the necessary links, we can move on to downloading the images. In order to do this, we will us the fast.ai library that has a module allowing us to download image files with ease\n",
    "\n",
    "https://www.geeksforgeeks.org/reading-writing-text-files-python/: working with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a quick .txt file containing the links, we will need it to give to the fastai lib\n",
    "\n",
    "\n",
    "test_links = []\n",
    "for links in img_links:\n",
    "    test_links.extend(links)\n",
    "\n",
    "test_links_content = \"\\n\".join(test_links)\n",
    "\n",
    "#open in write-only mode to truncate the existing links\n",
    "test_file = open(\"links.txt\",\"w\")\n",
    "test_file.write(test_links_content)\n",
    "test_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from fastai.vision import download_images\n",
    "\n",
    "dest_path = \"../../assets/assetsv2/\"\n",
    "links = \"links.txt\"\n",
    "\n",
    "\n",
    "download_images(links,dest_path)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
